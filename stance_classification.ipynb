{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torchtext\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer\n",
    "from ibm_dataset import IBMDebater\n",
    "import utils\n",
    "from train_text import train_loop\n",
    "from models.text_model import TextModel\n",
    "from models.audio_model import AudioModel\n",
    "from models.multimodal_model import MultimodalModel\n",
    "from transformers import DistilBertTokenizer\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from early_stopping import EarlyStopping\n",
    "from train import train_loop\n",
    "import os\n",
    "transformers.logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg_file = \"config/multimodal.yaml\"\n",
    "cfg = config.get_cfg_defaults()\n",
    "cfg.merge_from_file(cfg_file)\n",
    "cfg.freeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = cfg.MODEL.NAME\n",
    "device = cfg.SETTINGS.DEVICE\n",
    "data_path = cfg.DATASET.DATA_PATH\n",
    "load_audio = cfg.DATASET.LOAD_AUDIO\n",
    "load_text = cfg.DATASET.LOAD_TEXT\n",
    "chunk_length = cfg.DATASET.CHUNK_LENGTH\n",
    "text_transform = torchtext.transforms.ToTensor()\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(cfg.DATASET.TOKENIZER)\n",
    "\n",
    "\n",
    "data = IBMDebater(data_path, \n",
    "                  split='train', \n",
    "                  tokenizer=tokenizer, \n",
    "                  max_audio_len=chunk_length, \n",
    "                  text_transform=text_transform,\n",
    "                  load_audio=load_audio,\n",
    "                  load_text=load_text)\n",
    "\n",
    "train_len = int(len(data)*0.7)\n",
    "\n",
    "if cfg.DATASET.SMALL_VERSION:\n",
    "    small_data_dim = 0.2\n",
    "    rnd_idx = np.random.choice(np.array([i for i in range(1, len(data))]), size=int(len(data)*small_data_dim))\n",
    "    small_data = torch.utils.data.Subset(data, rnd_idx)\n",
    "    train_len = int(len(small_data)*0.7) \n",
    "    data_train, data_val = random_split(small_data, [train_len, len(small_data) - train_len])\n",
    "else:\n",
    "    data_train, data_val = random_split(data, [train_len, len(data) - train_len])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_name == 'text':\n",
    "    collate_fn = utils.batch_generator_text\n",
    "elif model_name == 'audio':\n",
    "    collate_fn = utils.batch_generator_wav2vec\n",
    "else:\n",
    "    collate_fn = utils.batch_generator_multimodal\n",
    "\n",
    "batch_size = cfg.DATASET.LOADER.BATCH_SIZE\n",
    "drop_last = cfg.DATASET.LOADER.DROP_LAST\n",
    "loader_train = DataLoader(data_train,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_fn,\n",
    "                    drop_last=drop_last)\n",
    "loader_val = DataLoader(data_val,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False,\n",
    "                    collate_fn=collate_fn,\n",
    "                    drop_last=drop_last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(cfg):\n",
    "    model = None\n",
    "    model_name = cfg.MODEL.NAME\n",
    "    models = []\n",
    "    if model_name == 'text' or model_name == 'multimodal':\n",
    "        models.append(TextModel(\n",
    "                            distilbert_type=cfg.MODEL.TEXT.DISTILBERT,\n",
    "                            n_trainable_layers=cfg.MODEL.TEXT.N_TRAINABLE_LAYERS,\n",
    "                            p_list=cfg.MODEL.TEXT.DROPOUT_VALUES,\n",
    "                            pre_classifier=cfg.MODEL.TEXT.PRE_CLASSIFIER,\n",
    "                            classify=cfg.MODEL.TEXT.CLASSIFY\n",
    "                        )\n",
    "                    )\n",
    "    if model_name == 'audio' or model_name == 'multimodal':\n",
    "        models.append(AudioModel(\n",
    "                            chunk_length=cfg.DATASET.CHUNK_LENGTH, \n",
    "                            downsampler_out_dim=cfg.MODEL.AUDIO.DOWNSAMPLER_OUT_DIM,\n",
    "                            n_trainable_layers=cfg.MODEL.AUDIO.N_TRAINABLE_LAYERS,\n",
    "                            bilstm_hidden_size=cfg.MODEL.AUDIO.BILSTM_HIDDEN_SIZE,\n",
    "                            device=cfg.SETTINGS.DEVICE,\n",
    "                            p_list=cfg.MODEL.AUDIO.DROPOUT_VALUES,\n",
    "                            pre_classifier=cfg.MODEL.AUDIO.PRE_CLASSIFIER,\n",
    "                            classify=cfg.MODEL.AUDIO.CLASSIFY\n",
    "                        )\n",
    "                    )\n",
    "    if cfg.MODEL.NAME == 'multimodal':\n",
    "        if cfg.MODEL.MULTIMODAL.LOAD_TEXT_CHECKPOINT:\n",
    "            models[0].load_backbone(cfg.MODEL.MULTIMODAL.TEXT_CHECPOINT_PATH, drop_classifier=True)\n",
    "        if cfg.MODEL.MULTIMODAL.LOAD_AUDIO_CHECKPOINT:\n",
    "            models[1].load_backbone(cfg.MODEL.MULTIMODAL.AUDIO_CHECPOINT_PATH, drop_classifier=True)\n",
    "        model = MultimodalModel(\n",
    "                        text_model=models[0],\n",
    "                        audio_model=models[1],\n",
    "                        p_list=cfg.MODEL.MULTIMODAL.DROPOUT_VALUES,\n",
    "                        freeze_text=cfg.MODEL.MULTIMODAL.FREEZE_TEXT,\n",
    "                        freeze_audio=cfg.MODEL.MULTIMODAL.FREEZE_AUDIO\n",
    "                    )\n",
    "    else:\n",
    "        model = models[0]\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model(cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = cfg.TRAIN.OPTIMIZER\n",
    "optimizer_args = cfg.TRAIN.OPTIMIZER_ARGS\n",
    "scheduler = cfg.TRAIN.LR_SCHEDULER\n",
    "early_stopping = cfg.TRAIN.EARLY_STOPPING\n",
    "lr = cfg.TRAIN.LR\n",
    "epochs = cfg.TRAIN.EPOCHS\n",
    "params = [{'params': model.parameters(), 'lr':lr}]\n",
    "if len(optimizer_args) > 0:\n",
    "    params = utils.get_params_groups(model, optimizer_args)\n",
    "if optimizer == 'adam':\n",
    "    optimizer = optim.Adam(params, lr=lr)\n",
    "if len(scheduler) > 0:\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, **scheduler)\n",
    "early_stopping = EarlyStopping(model, patience=early_stopping.PATIENCE)\n",
    "criterion = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(model, optimizer, criterion, early_stopping, loader_train, loader_val, epochs, device, step_lr=scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.TRAIN.SAVE_CHECKPOINT:\n",
    "    path = cfg.TRAIN.CHECKPOINT_PATH\n",
    "    model.save_backbone(path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "163f9789a5a304d7da8b497e35078abf9819c9a8bd180034b67bc521774fa763"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 ('env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
